# Results
# Yunlong Jiao, 15 Apr 2016

This script collects results from earlier run and illustrates with tables and plots.

```{r setup, message = FALSE}
knitr::opts_chunk$set(error = FALSE, warning = FALSE, message = FALSE, fig.width = 12, fig.height = 8, dev = c("png","pdf"), fig.keep = "high", fig.path = "result_figure/", cache.path = "result_cache/")
set.seed(70236562)
source("../../src/func.R")
library(ggplot2)
```

Read in parameters.

```{r param}
# read in parameters
param <- read.table("cluster_param.txt", header = FALSE, row.names = NULL, col.names = c("idx", "xname", "yname", "prname", "rep", "nfolds", "nrepeats"))
xlist <- unique(param$xname)
xlist
ylist <- unique(param$yname)
ylist
prlist <- unique(param$prname)
prlist
nfolds <- unique(param$nfolds)
stopifnot(length(nfolds) == 1)
nfolds
nrepeats <- unique(param$nrepeats)
stopifnot(length(nrepeats) == 1)
nrepeats
slist <- c("acc","fpr","tpr","ppv","fval","auroc")
slist
```

Read in CV folds results.

```{r prepare, cache=TRUE}
# gather scores
scores <- list()
for (xname in xlist) {
  for (yname in ylist){
    for (prname in prlist) {
      message(".", appendLF = FALSE)
      objname <- paste('res', xname, yname, prname, nfolds, nrepeats, sep = '_')
      # read and combine cv results from cluster runs
      cvres.files <- list.files(path = "Robj", pattern = objname, full.names = TRUE)
      nfiles <- length(cvres.files) # number of cv jobs successfully done
      cvres <- lapply(cvres.files, function(f) get(load(f)))
      names(cvres) <- cvres.files
      assign(objname, cvres)
      # plot ROC
      cvres <- crossValidationCombineResults(cvres)
      if (!dir.exists('figures')) dir.create('figures')
      plotROCcv(res = cvres, savepath = paste0('figures/', objname, '.pdf'))
      # write in scores
      ss <- cvres[slist]
      ss[sapply(ss, is.null)] <- NA
      scores[[objname]] <- data.frame(x = xname, 
                                      y = yname, 
                                      predictor = prname, 
                                      score = slist, 
                                      value = unlist(ss), 
                                      n.avg.cvfolds = nfiles,
                                      row.names = NULL)
    }
  }
}
scores <- do.call('rbind', scores)
rownames(scores) <- seq(nrow(scores))
# write out scores
write.table(scores, file = "scores.txt", row.names = TRUE, col.names = TRUE, sep = '\t')
# preview
head(scores)
```

# Overview

One plot corresponds to predicting for one type of phenotypic responses. In each plot, along x-axis we have different feature matrices, along y-axis we have different evaluation measures, the barplot shows the maximum CV scores from different predictors indicating the best predictor suited for such features.

Notes:

- `cutoff` threshold for predicted probability is always set constant 0.5, although it might be interesting to tune it in case that the class sizes in training data are unbalanced.
- The CV evaluated scores are overfitting typically for models with tuning parameters. A decent solution would be to set `prname` as one of the tuning parameters in nested CV call, or simply use another independent dataset to evaluate the scores.

```{r overview}
# plot each grps in a separate figure
for (yname in ylist) {
  d <- subset(scores, scores$y == yname)
  p1 <- ggplot(d, aes(x = x, y = value)) + 
    stat_summary_bin(aes(fill = x), fun.y = "max", geom = "bar") + 
    stat_summary(aes(label = round(..y.., 3)), fun.y = "max", geom = "text", size = 4,
                 colour = "black", position = position_dodge(0.9), vjust = -0.3) + 
    facet_wrap(~score) + 
    coord_cartesian(ylim = c(0, 1)) + 
    ggtitle(paste0("predicting ", yname)) + 
    theme(axis.text.x = element_blank())
  plot(p1)
}
```

# AUROC

As we see that class sizes are unbalanced so that we focus on AUROC to evaluate performance.

One plot corresponds to predicting for one type of phenotypic responses. In each plot, along x-axis we have different feature matrices, along y-axis we have CV AUROC scores, the boxplot shows the variance from different CV folds indicating the performance when data is randomly split.

```{r auroc}
# focus only one type of scores
sname <- "auroc"
# plot each grps in a separate figure
for (yname in ylist) {
  # gather scores
  d <- list()
  for (xname in xlist) {
    for (prname in prlist) {
      objname <- paste('res', xname, yname, prname, nfolds, nrepeats, sep = '_')
      cvres <- get(objname)
      ss <- lapply(cvres, function(u) u[[sname]])
      ss[sapply(ss, is.null)] <- NA
      d[[objname]] <- data.frame(x = xname, 
                                 y = yname, 
                                 predictor = prname, 
                                 score = sname, 
                                 value = unlist(ss), 
                                 rep = 1:length(ss), 
                                 row.names = NULL)
    }
  }
  d <- do.call('rbind', d)
  rownames(d) <- seq(nrow(d))
  # plot
  p1 <- ggplot(d, aes(x = x, y = value)) + 
    geom_boxplot(aes(fill = x), alpha = 0.8) + 
    facet_wrap(~predictor) + 
    ggtitle(paste0(sname, " for predicting ", yname)) + 
    theme(axis.text.x = element_blank())
  plot(p1)
  ### TODO t-test on boxplot to quantify superiority of one feature type over another?
}
```

# Session info

```{r session_info}
sessionInfo()
```
